### Import all spark modules and libraries needed

from pyspark.sql import SparkSession 
from pyspark.sql.types import *
import pyspark.sql.functions as F
from pyspark.sql.functions import col
spark = SparkSession.builder.getOrCreate()

### create Spark session

spark = SparkSession.builder.getOrCreate()

### Import the csv files and creating a dataframe for the two datasets and infer schema

file_location1 = "weather.20160201.csv"
file_location2 = "weather.20160301.csv"
file_type = "csv"

# CSV options

infer_schema = "True"
first_row_is_header = "True"
delimiter = ","

WeatherSchema = StructType([
  StructField("ForecastSiteCode",IntegerType()),
  StructField("Time",IntegerType()),
  StructField("Date",TimestampType()),
  StructField("WindDirection",IntegerType()),
  StructField("WindSpeed",IntegerType()),
  StructField("WindGust",IntegerType()),
  StructField("Visibility",IntegerType()),
  StructField("Temperature",DoubleType()),
  StructField("Pressure",IntegerType()),
  StructField("WeatherCode",IntegerType()),
  StructField("SiteName",StringType()),
  StructField("Latitude",DoubleType()),
  StructField("Longitude",DoubleType()),
  StructField("Region",StringType()),
  StructField("COUNTRY",StringType())
])

df1 = spark.read.format(file_type) \
  .schema(WeatherSchema)\
  .option("header", first_row_is_header) \
  .option("mode", "failFast") \
  .option("sep", delimiter) \
  .load(file_location1)

df1.count()

### Get the number of partitions in the dataset

print(df1.rdd.getNumPartitions(), df1.rdd.getNumPartitions())

### write the file in parquet file for optimisation

weatherDataCombined = df1.union(df2)
weatherDataCombined.write.format("parquet").mode("overwrite").save("weatherDataCombined.parquet")
weatherData = weather.union(weather2)

##  Creating a temporary view table

weather = spark.read.parquet("weatherDataCombined.parquet") 
weather.createOrReplaceTempView('weather')           
spark.catalog.cacheTable("weather")
weather.count()

#-----------Querying the weatherData to solve the solutions---------

spark.sql("SELECT Region,Date,Temperature FROM weather WHERE Temperature IN (SELECT MAX(Temperature) FROM weather)").show()

## Getting the maximum temperature by date and origin

maxTemperature = weather.selectExpr("max(Temperature)")
maxTemperature.show()

Table1 = weather.select("Date", "Region", "Temperature")
Table1.filter(col("Temperature") == 15.8).show()

